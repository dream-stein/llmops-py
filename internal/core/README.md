大语言模型的上下问长度 = 
预设的提示词长度 1000
+短期记忆的长度 2000
+长期记忆的长度 2000
+用户原始提问query的长度 2000
+工具调用/知识库检索占用的长度 10000
+大模型剩余可以生成的长度

gpt-4o-mini: 32k = 32000个token
预设的部分已经占据了 17000
剩余可以生成内容的长度其实只有 15000个token